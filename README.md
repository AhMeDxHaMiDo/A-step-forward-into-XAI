# The project is still under development, here is the first concluded results that I obtained so far (which is promising!).
If you are interested in collaboration, please message me on [LinkedIn](https://www.linkedin.com/in/ahmedxomar101/).
# Analysis-of-multi-class-vs-multi-task-classification
This project is one of my personal projects that I decided to move it forward and work on it for the Advanced Machine Learning Course at Sapienza University under the supervision of Professor Fabio Galasso.

## Introduction
Deep Learning achieved superhuman performance on Vision Multi-Class classification tasks but lacking interpretability and explainability of the decisions made by the neural network. Hence, make it difficult to interpret or investigate why the network took such a decision. Barring most deep learning methods from use in fields that demand accurate and justifiable predictions such as finance and medicine. In response, previous work combines hierarchical classification with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. In our proposed architecture design, we overcome this dilemma by jointly improving accuracy and interpretability using (1) a pre-trained backbone combined with (2) a multi-output architecture that performs separate classification tasks according to (3) the hierarchical tree generated by WordNet to interpret the origin categories of the target classes. As well as, generalizing to unseen classes, since the model is able to predict the origin categories (hierarchy) of any unseen class.

## BACKGROUND
This project is a combination of three ideas.
1. Performing a hierarchical decisions using multiple classifier nodes.
    * NEURAL-BACKED DECISION TREE – [Alvin Wan et al., 2021]
2. Multi-Output/Task Learning
    * The aim of multi-output learning is to simultaneously predict multiple outputs at once given an input. It is an important learning problem for decision-making.
    * A Survey on Multi-output Learning – [Donna Xu et al., 2019]
3. Leveraging a strong backbone and/or the pre-trained weights as a feature extractor, then sharing them with task-specific output heads3.
    * MULTI-TASK LEARNING WITH DEEP NEURAL NETWORKS: A SURVEY – [Michael Crawshaw, 2020]

## MAIN GOAL
Leveraging the previous combination to achieve Explainable AI (XAI) without losing performance.

![asset_01](https://user-images.githubusercontent.com/44009418/208243116-29b27a7a-f083-4e20-a60d-eabd3117b10c.JPG)

## SETUP
The main highlight is customizing the DataLoader of pytorch to load the label hierarchy that is generated by WordNet.

![image](https://user-images.githubusercontent.com/44009418/208243375-81b7d8c6-aef3-42f6-a46f-cadd16b1d107.png)

## Dataset
1. [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)
![image](https://user-images.githubusercontent.com/44009418/208243512-c8189bd2-a024-48be-aca8-c169347a5d13.png)

Basically, working os this dataset was just a hypothesis testing of the idea, and since it seems promising, the next step is to work on CIFAR-10 and compare the performacne to the benchmark then move forward from there.

## Architecture
![image](https://user-images.githubusercontent.com/44009418/208243620-802ea0bd-6801-40d0-9577-b92380fc24f6.png)

## Test set Results 
![image](https://user-images.githubusercontent.com/44009418/208243659-536febe5-3dd0-4bec-965c-58eed811813d.png)

## Findings
![image](https://user-images.githubusercontent.com/44009418/208243734-4a561a92-cd43-4563-9c5f-0f4b425e56d7.png)

## LEARNED LESSONS
![image](https://user-images.githubusercontent.com/44009418/208243741-5761e65d-b342-4aaf-8388-730759417ae8.png)

## References
![image](https://user-images.githubusercontent.com/44009418/208243751-84875ea5-77b4-4ab7-b502-7c6ee66482b1.png)
